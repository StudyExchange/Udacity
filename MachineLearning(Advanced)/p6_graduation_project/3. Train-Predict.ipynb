{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train-Predict\n",
    "\n",
    "**Result:**\n",
    "- Kaggle score: \n",
    "\n",
    "**Tensorboard**\n",
    "- Input at command: tensorboard --logdir=./log\n",
    "- Input at browser: http://127.0.0.1:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_name: Dog_Breed_Identification_Train-Predict_20180223_134540\n",
      "log_path: \tD:\\Udacity\\MachineLearning(Advanced)\\p6_graduation_project\\log\n",
      "model_path: \tD:\\Udacity\\MachineLearning(Advanced)\\p6_graduation_project\\model\n",
      "output_path: \tD:\\Udacity\\MachineLearning(Advanced)\\p6_graduation_project\\output\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "project_name = 'Dog_Breed_Identification'\n",
    "step_name = 'Train-Predict'\n",
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "run_name = project_name + '_' + step_name + '_' + time_str\n",
    "print('run_name: ' + run_name)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "log_path = os.path.join(cwd, 'log')\n",
    "model_path = os.path.join(cwd, 'model')\n",
    "output_path = os.path.join(cwd, 'output')\n",
    "print('log_path: \\t' + log_path)\n",
    "print('model_path: \\t' + model_path)\n",
    "print('output_path: \\t' + output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lables amount: 10222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id             breed\n",
       "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
       "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
       "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
       "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
       "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(cwd, 'input', 'labels.csv'))\n",
    "print('lables amount: %d' %len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30290, 5632)\n",
      "(5632,)\n",
      "----------\n",
      "(30290, 5632)\n",
      "30290\n",
      "(512, 5632)\n",
      "512\n",
      "(10357, 5632)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "np.random.seed(2017)\n",
    "\n",
    "x_train = []\n",
    "y_train = {}\n",
    "x_val = []\n",
    "y_val = {}\n",
    "x_test = []\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# feature_cgg16 = os.path.join(cwd, 'model', 'feature_VGG16_{}.h5'.format(20180219))\n",
    "# feature_cgg19 = os.path.join(cwd, 'model', 'feature_VGG19_{}.h5'.format(20180219))\n",
    "# feature_resnet50 = os.path.join(cwd, 'model', 'feature_ResNet50_{}.h5'.format(20180220))\n",
    "feature_xception = os.path.join(cwd, 'model', 'feature_Xception_{}.h5'.format(20180221))\n",
    "feature_inception = os.path.join(cwd, 'model', 'feature_InceptionV3_{}.h5'.format(20180221))\n",
    "feature_inceptionResNetV2 = os.path.join(cwd, 'model', 'feature_InceptionResNetV2_{}.h5'.format(20180221))\n",
    "# for filename in [feature_cgg16, feature_cgg19, feature_resnet50, feature_xception, feature_inception, feature_inceptionResNetV2]:\n",
    "for filename in [feature_xception, feature_inception, feature_inceptionResNetV2]:\n",
    "# for filename in [feature_inception]:\n",
    "    with h5py.File(filename, 'r') as h:\n",
    "        x_train.append(np.array(h['train']))\n",
    "        y_train = np.array(h['train_labels'])\n",
    "        x_val.append(np.array(h['val']))\n",
    "        y_val = np.array(h['val_labels'])\n",
    "        x_test.append(np.array(h['test']))\n",
    "\n",
    "# print(x_train[0].shape)\n",
    "x_train = np.concatenate(x_train, axis=-1)\n",
    "# y_train = np.concatenate(y_train, axis=0)\n",
    "x_val = np.concatenate(x_val, axis=-1)\n",
    "# y_val = np.concatenate(y_val, axis=0)\n",
    "x_test = np.concatenate(x_test, axis=-1)\n",
    "print(x_train.shape)\n",
    "print(x_train.shape[1:])\n",
    "print('-' * 10)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(len(y_train))\n",
    "print(x_val.shape)\n",
    "print(len(y_val))\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "(x_train, y_train) = shuffle(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30290, 5632)\n",
      "(30290,)\n",
      "(512, 5632)\n",
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.025, random_state=5)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30290, 120)\n",
      "(512, 120)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_dir:D:\\Udacity\\MachineLearning(Advanced)\\p6_graduation_project\\log\\Dog_Breed_Identification_Train-Predict_20180223_134540\n"
     ]
    }
   ],
   "source": [
    "def get_lr(x):\n",
    "    lr = round(3e-4 * 0.96 ** x, 10)\n",
    "    if lr < 1e-8:\n",
    "        lr = 1e-8\n",
    "    print('%.12f' % lr, end='  ')\n",
    "    return lr\n",
    "\n",
    "# annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n",
    "annealer = LearningRateScheduler(get_lr)\n",
    "\n",
    "log_dir = os.path.join(log_path, run_name)\n",
    "print('log_dir:' + log_dir)\n",
    "tensorBoard = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4096, input_shape=x_train.shape[1:]))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4096, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4096, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "# model.add(Dense(1024, activation='sigmoid'))\n",
    "# model.add(Dropout(0.3))\n",
    "model.add(Dense(120, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4096)              23072768  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 120)               491640    \n",
      "=================================================================\n",
      "Total params: 57,127,032\n",
      "Trainable params: 57,127,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30290 samples, validate on 512 samples\n",
      "0.000300000000  Epoch 1/300\n",
      "30290/30290 [==============================] - 103s 3ms/step - loss: 1.2248 - acc: 0.7437 - val_loss: 0.3038 - val_acc: 0.9043\n",
      "0.000288000000  Epoch 2/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.2692 - acc: 0.9208 - val_loss: 0.2561 - val_acc: 0.9297\n",
      "0.000276480000  Epoch 3/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.2186 - acc: 0.9336 - val_loss: 0.2057 - val_acc: 0.9395\n",
      "0.000265420800  Epoch 4/300\n",
      "30290/30290 [==============================] - 103s 3ms/step - loss: 0.1937 - acc: 0.9409 - val_loss: 0.2051 - val_acc: 0.9375\n",
      "0.000254804000  Epoch 5/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.1690 - acc: 0.9469 - val_loss: 0.1798 - val_acc: 0.9512\n",
      "0.000244611800  Epoch 6/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.1460 - acc: 0.9538 - val_loss: 0.1688 - val_acc: 0.9473\n",
      "0.000234827300  Epoch 7/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.1253 - acc: 0.9586 - val_loss: 0.1412 - val_acc: 0.9473\n",
      "0.000225434200  Epoch 8/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.1120 - acc: 0.9647 - val_loss: 0.1518 - val_acc: 0.9531\n",
      "0.000216416900  Epoch 9/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.1045 - acc: 0.9662 - val_loss: 0.1568 - val_acc: 0.9512\n",
      "0.000207760200  Epoch 10/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0895 - acc: 0.9714 - val_loss: 0.1227 - val_acc: 0.9590\n",
      "0.000199449800  Epoch 11/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0779 - acc: 0.9753 - val_loss: 0.1144 - val_acc: 0.9609\n",
      "0.000191471800  Epoch 12/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0703 - acc: 0.9771 - val_loss: 0.0809 - val_acc: 0.9727\n",
      "0.000183812900  Epoch 13/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0623 - acc: 0.9786 - val_loss: 0.0820 - val_acc: 0.9707\n",
      "0.000176460400  Epoch 14/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0580 - acc: 0.9810 - val_loss: 0.0766 - val_acc: 0.9727\n",
      "0.000169402000  Epoch 15/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0471 - acc: 0.9848 - val_loss: 0.0791 - val_acc: 0.9707\n",
      "0.000162625900  Epoch 16/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0431 - acc: 0.9860 - val_loss: 0.0563 - val_acc: 0.9844\n",
      "0.000156120900  Epoch 17/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0396 - acc: 0.9870 - val_loss: 0.0630 - val_acc: 0.9824\n",
      "0.000149876000  Epoch 18/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0373 - acc: 0.9877 - val_loss: 0.0572 - val_acc: 0.9844\n",
      "0.000143881000  Epoch 19/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0341 - acc: 0.9883 - val_loss: 0.0386 - val_acc: 0.9863\n",
      "0.000138125800  Epoch 20/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0281 - acc: 0.9909 - val_loss: 0.0445 - val_acc: 0.9883\n",
      "0.000132600700  Epoch 21/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0267 - acc: 0.9909 - val_loss: 0.0433 - val_acc: 0.9844\n",
      "0.000127296700  Epoch 22/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0226 - acc: 0.9930 - val_loss: 0.0372 - val_acc: 0.9824\n",
      "0.000122204800  Epoch 23/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0226 - acc: 0.9930 - val_loss: 0.0393 - val_acc: 0.9844\n",
      "0.000117316600  Epoch 24/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0203 - acc: 0.9935 - val_loss: 0.0182 - val_acc: 0.9941\n",
      "0.000112624000  Epoch 25/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0186 - acc: 0.9935 - val_loss: 0.0336 - val_acc: 0.9863\n",
      "0.000108119000  Epoch 26/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0194 - acc: 0.9938 - val_loss: 0.0352 - val_acc: 0.9883\n",
      "0.000103794300  Epoch 27/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0174 - acc: 0.9945 - val_loss: 0.0303 - val_acc: 0.9902\n",
      "0.000099642500  Epoch 28/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0176 - acc: 0.9941 - val_loss: 0.0258 - val_acc: 0.9883\n",
      "0.000095656800  Epoch 29/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0133 - acc: 0.9959 - val_loss: 0.0163 - val_acc: 0.9941\n",
      "0.000091830500  Epoch 30/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0121 - acc: 0.9962 - val_loss: 0.0329 - val_acc: 0.9922\n",
      "0.000088157300  Epoch 31/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0126 - acc: 0.9963 - val_loss: 0.0218 - val_acc: 0.9902\n",
      "0.000084631000  Epoch 32/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0113 - acc: 0.9967 - val_loss: 0.0178 - val_acc: 0.9961\n",
      "0.000081245800  Epoch 33/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0124 - acc: 0.9964 - val_loss: 0.0173 - val_acc: 0.9941\n",
      "0.000077995900  Epoch 34/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0107 - acc: 0.9966 - val_loss: 0.0249 - val_acc: 0.9883\n",
      "0.000074876100  Epoch 35/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0108 - acc: 0.9968 - val_loss: 0.0175 - val_acc: 0.9941\n",
      "0.000071881000  Epoch 36/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0103 - acc: 0.9966 - val_loss: 0.0164 - val_acc: 0.9961\n",
      "0.000069005800  Epoch 37/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0103 - acc: 0.9971 - val_loss: 0.0133 - val_acc: 0.9941\n",
      "0.000066245600  Epoch 38/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0088 - acc: 0.9973 - val_loss: 0.0234 - val_acc: 0.9941\n",
      "0.000063595800  Epoch 39/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0089 - acc: 0.9973 - val_loss: 0.0169 - val_acc: 0.9941\n",
      "0.000061051900  Epoch 40/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0081 - acc: 0.9973 - val_loss: 0.0200 - val_acc: 0.9922\n",
      "0.000058609800  Epoch 41/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0086 - acc: 0.9972 - val_loss: 0.0188 - val_acc: 0.9941\n",
      "0.000056265500  Epoch 42/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0083 - acc: 0.9975 - val_loss: 0.0108 - val_acc: 0.9941\n",
      "0.000054014800  Epoch 43/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0075 - acc: 0.9980 - val_loss: 0.0190 - val_acc: 0.9961\n",
      "0.000051854200  Epoch 44/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.0136 - val_acc: 0.9941\n",
      "0.000049780100  Epoch 45/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0081 - acc: 0.9974 - val_loss: 0.0118 - val_acc: 0.9941\n",
      "0.000047788900  Epoch 46/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0075 - acc: 0.9976 - val_loss: 0.0232 - val_acc: 0.9941\n",
      "0.000045877300  Epoch 47/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0174 - val_acc: 0.9941\n",
      "0.000044042200  Epoch 48/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0069 - acc: 0.9976 - val_loss: 0.0117 - val_acc: 0.9961\n",
      "0.000042280500  Epoch 49/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0138 - val_acc: 0.9902\n",
      "0.000040589300  Epoch 50/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0140 - val_acc: 0.9922\n",
      "0.000038965700  Epoch 51/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0063 - acc: 0.9979 - val_loss: 0.0160 - val_acc: 0.9941\n",
      "0.000037407100  Epoch 52/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0059 - acc: 0.9978 - val_loss: 0.0073 - val_acc: 0.9980\n",
      "0.000035910800  Epoch 53/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0061 - acc: 0.9979 - val_loss: 0.0076 - val_acc: 0.9980\n",
      "0.000034474400  Epoch 54/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0058 - acc: 0.9981 - val_loss: 0.0120 - val_acc: 0.9961\n",
      "0.000033095400  Epoch 55/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0058 - acc: 0.9980 - val_loss: 0.0135 - val_acc: 0.9961\n",
      "0.000031771600  Epoch 56/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0059 - acc: 0.9977 - val_loss: 0.0091 - val_acc: 0.9961\n",
      "0.000030500700  Epoch 57/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0068 - acc: 0.9976 - val_loss: 0.0128 - val_acc: 0.9961\n",
      "0.000029280700  Epoch 58/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0056 - acc: 0.9979 - val_loss: 0.0109 - val_acc: 0.9941\n",
      "0.000028109500  Epoch 59/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0057 - acc: 0.9979 - val_loss: 0.0156 - val_acc: 0.9941\n",
      "0.000026985100  Epoch 60/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0050 - acc: 0.9980 - val_loss: 0.0124 - val_acc: 0.9961\n",
      "0.000025905700  Epoch 61/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0050 - acc: 0.9981 - val_loss: 0.0097 - val_acc: 0.9941\n",
      "0.000024869500  Epoch 62/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0049 - acc: 0.9980 - val_loss: 0.0097 - val_acc: 0.9961\n",
      "0.000023874700  Epoch 63/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0050 - acc: 0.9979 - val_loss: 0.0130 - val_acc: 0.9922\n",
      "0.000022919700  Epoch 64/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0052 - acc: 0.9980 - val_loss: 0.0120 - val_acc: 0.9922\n",
      "0.000022002900  Epoch 65/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0052 - acc: 0.9982 - val_loss: 0.0094 - val_acc: 0.9961\n",
      "0.000021122800  Epoch 66/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0051 - acc: 0.9982 - val_loss: 0.0098 - val_acc: 0.9941\n",
      "0.000020277900  Epoch 67/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0049 - acc: 0.9981 - val_loss: 0.0113 - val_acc: 0.9941\n",
      "0.000019466800  Epoch 68/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0047 - acc: 0.9981 - val_loss: 0.0123 - val_acc: 0.9941\n",
      "0.000018688100  Epoch 69/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0048 - acc: 0.9982 - val_loss: 0.0117 - val_acc: 0.9941\n",
      "0.000017940600  Epoch 70/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0047 - acc: 0.9980 - val_loss: 0.0109 - val_acc: 0.9941\n",
      "0.000017223000  Epoch 71/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0042 - acc: 0.9983 - val_loss: 0.0093 - val_acc: 0.9941\n",
      "0.000016534000  Epoch 72/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0043 - acc: 0.9982 - val_loss: 0.0106 - val_acc: 0.9941\n",
      "0.000015872700  Epoch 73/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0042 - acc: 0.9983 - val_loss: 0.0087 - val_acc: 0.9941\n",
      "0.000015237800  Epoch 74/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0045 - acc: 0.9981 - val_loss: 0.0078 - val_acc: 0.9941\n",
      "0.000014628300  Epoch 75/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0043 - acc: 0.9983 - val_loss: 0.0089 - val_acc: 0.9941\n",
      "0.000014043100  Epoch 76/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0041 - acc: 0.9983 - val_loss: 0.0104 - val_acc: 0.9941\n",
      "0.000013481400  Epoch 77/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0042 - acc: 0.9983 - val_loss: 0.0091 - val_acc: 0.9941\n",
      "0.000012942100  Epoch 78/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0042 - acc: 0.9982 - val_loss: 0.0139 - val_acc: 0.9922\n",
      "0.000012424500  Epoch 79/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0040 - acc: 0.9983 - val_loss: 0.0101 - val_acc: 0.9941\n",
      "0.000011927500  Epoch 80/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0041 - acc: 0.9982 - val_loss: 0.0085 - val_acc: 0.9941\n",
      "0.000011450400  Epoch 81/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0040 - acc: 0.9984 - val_loss: 0.0100 - val_acc: 0.9941\n",
      "0.000010992400  Epoch 82/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0039 - acc: 0.9983 - val_loss: 0.0074 - val_acc: 0.9961\n",
      "0.000010552700  Epoch 83/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0039 - acc: 0.9981 - val_loss: 0.0102 - val_acc: 0.9941\n",
      "0.000010130600  Epoch 84/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0038 - acc: 0.9983 - val_loss: 0.0094 - val_acc: 0.9941\n",
      "0.000009725300  Epoch 85/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0037 - acc: 0.9982 - val_loss: 0.0110 - val_acc: 0.9941\n",
      "0.000009336300  Epoch 86/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0040 - acc: 0.9983 - val_loss: 0.0074 - val_acc: 0.9941\n",
      "0.000008962900  Epoch 87/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0041 - acc: 0.9981 - val_loss: 0.0069 - val_acc: 0.9941\n",
      "0.000008604400  Epoch 88/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0041 - acc: 0.9981 - val_loss: 0.0099 - val_acc: 0.9941\n",
      "0.000008260200  Epoch 89/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0037 - acc: 0.9983 - val_loss: 0.0086 - val_acc: 0.9961\n",
      "0.000007929800  Epoch 90/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0039 - acc: 0.9981 - val_loss: 0.0086 - val_acc: 0.9941\n",
      "0.000007612600  Epoch 91/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0034 - acc: 0.9983 - val_loss: 0.0083 - val_acc: 0.9941\n",
      "0.000007308100  Epoch 92/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0040 - acc: 0.9983 - val_loss: 0.0076 - val_acc: 0.9941\n",
      "0.000007015800  Epoch 93/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0037 - acc: 0.9982 - val_loss: 0.0078 - val_acc: 0.9961\n",
      "0.000006735100  Epoch 94/300\n",
      "30290/30290 [==============================] - 103s 3ms/step - loss: 0.0038 - acc: 0.9983 - val_loss: 0.0087 - val_acc: 0.9941\n",
      "0.000006465700  Epoch 95/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0037 - acc: 0.9983 - val_loss: 0.0102 - val_acc: 0.9941\n",
      "0.000006207100  Epoch 96/300\n",
      "30290/30290 [==============================] - 103s 3ms/step - loss: 0.0039 - acc: 0.9981 - val_loss: 0.0095 - val_acc: 0.9941\n",
      "0.000005958800  Epoch 97/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0033 - acc: 0.9985 - val_loss: 0.0086 - val_acc: 0.9941\n",
      "0.000005720500  Epoch 98/300\n",
      "30290/30290 [==============================] - 103s 3ms/step - loss: 0.0036 - acc: 0.9983 - val_loss: 0.0086 - val_acc: 0.9941\n",
      "0.000005491600  Epoch 99/300\n",
      "30290/30290 [==============================] - 103s 3ms/step - loss: 0.0033 - acc: 0.9985 - val_loss: 0.0088 - val_acc: 0.9941\n",
      "0.000005272000  Epoch 100/300\n",
      "30290/30290 [==============================] - 103s 3ms/step - loss: 0.0033 - acc: 0.9985 - val_loss: 0.0086 - val_acc: 0.9941\n",
      "0.000005061100  Epoch 101/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0039 - acc: 0.9982 - val_loss: 0.0090 - val_acc: 0.9941\n",
      "0.000004858700  Epoch 102/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0038 - acc: 0.9981 - val_loss: 0.0079 - val_acc: 0.9941\n",
      "0.000004664300  Epoch 103/300\n",
      "30290/30290 [==============================] - 103s 3ms/step - loss: 0.0034 - acc: 0.9985 - val_loss: 0.0085 - val_acc: 0.9941\n",
      "0.000004477700  Epoch 104/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0036 - acc: 0.9982 - val_loss: 0.0083 - val_acc: 0.9941\n",
      "0.000004298600  Epoch 105/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0037 - acc: 0.9981 - val_loss: 0.0076 - val_acc: 0.9941\n",
      "0.000004126700  Epoch 106/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0034 - acc: 0.9983 - val_loss: 0.0077 - val_acc: 0.9941\n",
      "0.000003961600  Epoch 107/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0034 - acc: 0.9982 - val_loss: 0.0079 - val_acc: 0.9941\n",
      "0.000003803100  Epoch 108/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0036 - acc: 0.9983 - val_loss: 0.0076 - val_acc: 0.9941\n",
      "0.000003651000  Epoch 109/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0037 - acc: 0.9980 - val_loss: 0.0073 - val_acc: 0.9941\n",
      "0.000003505000  Epoch 110/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9986 - val_loss: 0.0083 - val_acc: 0.9941\n",
      "0.000003364800  Epoch 111/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9986 - val_loss: 0.0078 - val_acc: 0.9941\n",
      "0.000003230200  Epoch 112/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0077 - val_acc: 0.9941\n",
      "0.000003101000  Epoch 113/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0033 - acc: 0.9982 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000002976900  Epoch 114/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0033 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000002857900  Epoch 115/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9985 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000002743600  Epoch 116/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0035 - acc: 0.9983 - val_loss: 0.0082 - val_acc: 0.9941\n",
      "0.000002633800  Epoch 117/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9985 - val_loss: 0.0083 - val_acc: 0.9941\n",
      "0.000002528500  Epoch 118/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9984 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000002427300  Epoch 119/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9984 - val_loss: 0.0084 - val_acc: 0.9941\n",
      "0.000002330200  Epoch 120/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0079 - val_acc: 0.9941\n",
      "0.000002237000  Epoch 121/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 0.0082 - val_acc: 0.9941\n",
      "0.000002147500  Epoch 122/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0035 - acc: 0.9983 - val_loss: 0.0086 - val_acc: 0.9941\n",
      "0.000002061600  Epoch 123/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 0.0084 - val_acc: 0.9941\n",
      "0.000001979200  Epoch 124/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9984 - val_loss: 0.0078 - val_acc: 0.9941\n",
      "0.000001900000  Epoch 125/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0033 - acc: 0.9982 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000001824000  Epoch 126/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9986 - val_loss: 0.0076 - val_acc: 0.9941\n",
      "0.000001751000  Epoch 127/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9986 - val_loss: 0.0077 - val_acc: 0.9941\n",
      "0.000001681000  Epoch 128/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9983 - val_loss: 0.0079 - val_acc: 0.9941\n",
      "0.000001613800  Epoch 129/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0033 - acc: 0.9984 - val_loss: 0.0077 - val_acc: 0.9941\n",
      "0.000001549200  Epoch 130/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0033 - acc: 0.9983 - val_loss: 0.0074 - val_acc: 0.9941\n",
      "0.000001487200  Epoch 131/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.0077 - val_acc: 0.9941\n",
      "0.000001427800  Epoch 132/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0034 - acc: 0.9982 - val_loss: 0.0078 - val_acc: 0.9941\n",
      "0.000001370600  Epoch 133/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0073 - val_acc: 0.9941\n",
      "0.000001315800  Epoch 134/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0072 - val_acc: 0.9941\n",
      "0.000001263200  Epoch 135/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9984 - val_loss: 0.0075 - val_acc: 0.9941\n",
      "0.000001212700  Epoch 136/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9984 - val_loss: 0.0075 - val_acc: 0.9941\n",
      "0.000001164200  Epoch 137/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9986 - val_loss: 0.0078 - val_acc: 0.9941\n",
      "0.000001117600  Epoch 138/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0036 - acc: 0.9983 - val_loss: 0.0078 - val_acc: 0.9941\n",
      "0.000001072900  Epoch 139/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9984 - val_loss: 0.0077 - val_acc: 0.9941\n",
      "0.000001030000  Epoch 140/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0077 - val_acc: 0.9941\n",
      "0.000000988800  Epoch 141/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0077 - val_acc: 0.9941\n",
      "0.000000949200  Epoch 142/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0076 - val_acc: 0.9941\n",
      "0.000000911200  Epoch 143/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9984 - val_loss: 0.0078 - val_acc: 0.9941\n",
      "0.000000874800  Epoch 144/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0033 - acc: 0.9983 - val_loss: 0.0079 - val_acc: 0.9941\n",
      "0.000000839800  Epoch 145/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 0.0078 - val_acc: 0.9941\n",
      "0.000000806200  Epoch 146/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0033 - acc: 0.9983 - val_loss: 0.0077 - val_acc: 0.9941\n",
      "0.000000774000  Epoch 147/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9984 - val_loss: 0.0078 - val_acc: 0.9941\n",
      "0.000000743000  Epoch 148/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0033 - acc: 0.9983 - val_loss: 0.0077 - val_acc: 0.9941\n",
      "0.000000713300  Epoch 149/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9982 - val_loss: 0.0078 - val_acc: 0.9941\n",
      "0.000000684800  Epoch 150/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9986 - val_loss: 0.0077 - val_acc: 0.9941\n",
      "0.000000657400  Epoch 151/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0077 - val_acc: 0.9941\n",
      "0.000000631100  Epoch 152/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9985 - val_loss: 0.0079 - val_acc: 0.9941\n",
      "0.000000605800  Epoch 153/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9984 - val_loss: 0.0079 - val_acc: 0.9941\n",
      "0.000000581600  Epoch 154/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000558300  Epoch 155/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9986 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000536000  Epoch 156/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9984 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000514600  Epoch 157/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000494000  Epoch 158/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9984 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000474200  Epoch 159/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9984 - val_loss: 0.0082 - val_acc: 0.9941\n",
      "0.000000455200  Epoch 160/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000437000  Epoch 161/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0035 - acc: 0.9982 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000419600  Epoch 162/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9986 - val_loss: 0.0082 - val_acc: 0.9941\n",
      "0.000000402800  Epoch 163/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9986 - val_loss: 0.0084 - val_acc: 0.9941\n",
      "0.000000386700  Epoch 164/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0083 - val_acc: 0.9941\n",
      "0.000000371200  Epoch 165/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 0.0083 - val_acc: 0.9941\n",
      "0.000000356300  Epoch 166/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9984 - val_loss: 0.0083 - val_acc: 0.9941\n",
      "0.000000342100  Epoch 167/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0033 - acc: 0.9983 - val_loss: 0.0083 - val_acc: 0.9941\n",
      "0.000000328400  Epoch 168/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 0.0083 - val_acc: 0.9941\n",
      "0.000000315300  Epoch 169/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9986 - val_loss: 0.0083 - val_acc: 0.9941\n",
      "0.000000302700  Epoch 170/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9986 - val_loss: 0.0083 - val_acc: 0.9941\n",
      "0.000000290600  Epoch 171/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9983 - val_loss: 0.0082 - val_acc: 0.9941\n",
      "0.000000278900  Epoch 172/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000267800  Epoch 173/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9986 - val_loss: 0.0082 - val_acc: 0.9941\n",
      "0.000000257100  Epoch 174/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000246800  Epoch 175/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000236900  Epoch 176/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000227400  Epoch 177/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0027 - acc: 0.9987 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000218300  Epoch 178/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000209600  Epoch 179/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9986 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000201200  Epoch 180/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9987 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000193200  Epoch 181/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9985 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000185400  Epoch 182/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9984 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000178000  Epoch 183/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000170900  Epoch 184/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0027 - acc: 0.9987 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000164100  Epoch 185/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9984 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000157500  Epoch 186/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9984 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000151200  Epoch 187/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0027 - acc: 0.9987 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000145200  Epoch 188/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000139400  Epoch 189/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9984 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000133800  Epoch 190/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9984 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000128400  Epoch 191/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9986 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000123300  Epoch 192/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9983 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000118400  Epoch 193/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0027 - acc: 0.9987 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000113600  Epoch 194/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9986 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000109100  Epoch 195/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9983 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000104700  Epoch 196/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000100500  Epoch 197/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000096500  Epoch 198/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000092600  Epoch 199/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9986 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000088900  Epoch 200/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9985 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000085400  Epoch 201/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000082000  Epoch 202/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000078700  Epoch 203/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000075500  Epoch 204/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0033 - acc: 0.9983 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000072500  Epoch 205/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9983 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000069600  Epoch 206/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000066800  Epoch 207/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000064200  Epoch 208/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0027 - acc: 0.9988 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000061600  Epoch 209/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000059100  Epoch 210/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000056800  Epoch 211/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0033 - acc: 0.9984 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000054500  Epoch 212/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9987 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000052300  Epoch 213/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9983 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000050200  Epoch 214/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9983 - val_loss: 0.0081 - val_acc: 0.9941\n",
      "0.000000048200  Epoch 215/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000046300  Epoch 216/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000044400  Epoch 217/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000042700  Epoch 218/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000040900  Epoch 219/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9983 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000039300  Epoch 220/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000037700  Epoch 221/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000036200  Epoch 222/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000034800  Epoch 223/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0033 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000033400  Epoch 224/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000032100  Epoch 225/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000030800  Epoch 226/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000029500  Epoch 227/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9983 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000028400  Epoch 228/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000027200  Epoch 229/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000026100  Epoch 230/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000025100  Epoch 231/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000024100  Epoch 232/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0027 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000023100  Epoch 233/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000022200  Epoch 234/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000021300  Epoch 235/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9983 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000020500  Epoch 236/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000019600  Epoch 237/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000018900  Epoch 238/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000018100  Epoch 239/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0033 - acc: 0.9983 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000017400  Epoch 240/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000016700  Epoch 241/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000016000  Epoch 242/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000015400  Epoch 243/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000014800  Epoch 244/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000014200  Epoch 245/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000013600  Epoch 246/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0027 - acc: 0.9988 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000013100  Epoch 247/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000012500  Epoch 248/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000012000  Epoch 249/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9982 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000011600  Epoch 250/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0027 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000011100  Epoch 251/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010600  Epoch 252/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010200  Epoch 253/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 254/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0027 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 255/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 256/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 257/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 258/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9983 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 259/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 260/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9982 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 261/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 262/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 263/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 264/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 265/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 266/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 267/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 268/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 269/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 270/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0033 - acc: 0.9983 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 271/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 272/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 273/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9982 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 274/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 275/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0033 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 276/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 277/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 278/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 279/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 280/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 281/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 282/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0026 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 283/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 284/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0031 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 285/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 286/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9984 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 287/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 288/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9987 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 289/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 290/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 291/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 292/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0027 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 293/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 294/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0033 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 295/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 296/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 297/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0029 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 298/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0027 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 299/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0080 - val_acc: 0.9941\n",
      "0.000000010000  Epoch 300/300\n",
      "30290/30290 [==============================] - 102s 3ms/step - loss: 0.0032 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9941\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train,\n",
    "                 batch_size=256,\n",
    "                 epochs=300, #Increase this when not on Kaggle kernel\n",
    "                 verbose=1,  #1 for ETA, 0 for silent\n",
    "                 callbacks=[annealer],\n",
    "                 validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 0s 820us/step\n",
      "Final loss: 0.0080, final accuracy: 0.9941\n"
     ]
    }
   ],
   "source": [
    "final_loss, final_acc = model.evaluate(x_val, y_val, verbose=1)\n",
    "print(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name_acc = run_name + '_' + str(int(final_loss*10000)).zfill(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['acc', 'loss', 'val_acc', 'val_loss', 'epoch'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "histories = pd.DataFrame(hist.history)\n",
    "histories['epoch'] = hist.epoch\n",
    "print(histories.columns)\n",
    "histories_file = os.path.join(model_path, run_name_acc + '.csv')\n",
    "histories.to_csv(histories_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9P8B1Nry+byA/2IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x259233e6828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x259c613ae10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(hist.history['loss'], color='b')\n",
    "plt.plot(hist.history['val_loss'], color='r')\n",
    "plt.show()\n",
    "plt.plot(hist.history['acc'], color='b')\n",
    "plt.plot(hist.history['val_acc'], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model, run_name):\n",
    "    cwd = os.getcwd()\n",
    "    modelPath = os.path.join(cwd, 'model')\n",
    "    if not os.path.isdir(modelPath):\n",
    "        os.mkdir(modelPath)\n",
    "    weigthsFile = os.path.join(modelPath, run_name + '.h5')\n",
    "    model.save(weigthsFile)\n",
    "saveModel(model, run_name_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to load model directly and skip train\n",
    "# import os\n",
    "# from keras.models import load_model\n",
    "# cwd = os.getcwd()\n",
    "# model = load_model(os.path.join(cwd, 'model', 'Dog_Breed_Identification_Train_20171024_155154.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10357, 120)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test, batch_size=128)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_pred[:10])\n",
    "# y_pred = np.clip(y_pred, 0.005, 0.995)\n",
    "# print(y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000621fb3cbb32d8935728e48679680e.jpg', '00102ee9d8eb90812350685311fe5890.jpg', '0012a730dfa437f5f3613fb75efcd4ce.jpg', '001510bc8570bbeee98c8d80c8a95ec1.jpg', '001a5f3114548acdefa3d4da05474c2e.jpg', '00225dcd3e4d2410dd53239f95c0352f.jpg', '002c2a3117c2193b4d26400ce431eebd.jpg', '002c58d413a521ae8d1a5daeb35fc803.jpg', '002f80396f1e3db687c5932d7978b196.jpg', '0036c6bcec6031be9e62a257b1c3c442.jpg']\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(os.path.join(cwd, 'input', 'data_test', 'test'))\n",
    "print(files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lables amount: 10222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id             breed\n",
       "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
       "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
       "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
       "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
       "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "df = pd.read_csv(os.path.join(cwd, 'input', 'labels.csv'))\n",
    "print('lables amount: %d' %len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'miniature_schnauzer', 'wire-haired_fox_terrier', 'norfolk_terrier', 'bluetick', 'sealyham_terrier', 'norwich_terrier', 'gordon_setter', 'miniature_poodle', 'bernese_mountain_dog', 'pug', 'dhole', 'toy_terrier', 'toy_poodle', 'tibetan_terrier', 'siberian_husky', 'keeshond', 'blenheim_spaniel', 'chihuahua', 'whippet', 'basenji', 'great_pyrenees', 'bedlington_terrier', 'west_highland_white_terrier', 'rhodesian_ridgeback', 'borzoi', 'entlebucher', 'kerry_blue_terrier', 'chesapeake_bay_retriever', 'brabancon_griffon', 'english_springer', 'irish_water_spaniel', 'french_bulldog', 'standard_poodle', 'cardigan', 'soft-coated_wheaten_terrier', 'scottish_deerhound', 'flat-coated_retriever', 'silky_terrier', 'black-and-tan_coonhound', 'german_short-haired_pointer', 'brittany_spaniel', 'weimaraner', 'mexican_hairless', 'pekinese', 'boxer', 'shih-tzu', 'welsh_springer_spaniel', 'saluki', 'cairn', 'schipperke', 'pomeranian', 'doberman', 'sussex_spaniel', 'scotch_terrier', 'english_setter', 'african_hunting_dog', 'otterhound', 'clumber', 'japanese_spaniel', 'malamute', 'affenpinscher', 'great_dane', 'yorkshire_terrier', 'beagle', 'australian_terrier', 'maltese_dog', 'tibetan_mastiff', 'standard_schnauzer', 'rottweiler', 'saint_bernard', 'greater_swiss_mountain_dog', 'eskimo_dog', 'dingo', 'miniature_pinscher', 'bull_mastiff', 'lakeland_terrier', 'papillon', 'irish_setter', 'basset', 'old_english_sheepdog', 'boston_bull', 'leonberg', 'staffordshire_bullterrier', 'pembroke', 'irish_terrier', 'kuvasz', 'cocker_spaniel', 'chow', 'border_terrier', 'malinois', 'curly-coated_retriever', 'border_collie', 'bloodhound', 'newfoundland', 'ibizan_hound', 'norwegian_elkhound', 'redbone', 'walker_hound', 'bouvier_des_flandres', 'italian_greyhound', 'giant_schnauzer', 'airedale', 'dandie_dinmont', 'briard', 'afghan_hound', 'golden_retriever', 'samoyed', 'german_shepherd', 'american_staffordshire_terrier', 'lhasa', 'english_foxhound', 'appenzeller', 'vizsla', 'irish_wolfhound', 'labrador_retriever', 'collie', 'kelpie', 'shetland_sheepdog', 'groenendael', 'komondor'}\n"
     ]
    }
   ],
   "source": [
    "n = len(df)\n",
    "breed = set(df['breed'])\n",
    "n_class = len(breed)\n",
    "class_to_num = dict(zip(breed, range(n_class)))\n",
    "num_to_class = dict(zip(range(n_class), breed))\n",
    "print(breed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10357, 121)\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('.\\\\input\\\\sample_submission.csv')\n",
    "n_test = len(df2)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10357, 120)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 120):\n",
    "    df2.iloc[:,[i+1]] = y_pred[:,i]\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "pred_file = os.path.join(output_path, 'pred_' + run_name_acc + '.csv')\n",
    "df2.to_csv(pred_file, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog_Breed_Identification_Train-Predict_20180223_134540_0080\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "print(run_name_acc)\n",
    "print('Done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
